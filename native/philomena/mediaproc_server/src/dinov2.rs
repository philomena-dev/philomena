use std::io::Cursor;
use tch::{CModule, Device, IValue, Tensor};

use super::io;
use crate::FeatureExtractionError;

/// Each DINOv2 patch is 14x14
pub const PATCH_DIM: i64 = 14;

#[allow(dead_code)]
pub struct ModelResult {
    pub patches: (i64, i64),
    pub image: Tensor,
    pub features: Tensor,
    pub last_hidden_state: Tensor,
}

fn infer(image: &Tensor, model: &CModule) -> (Tensor, Tensor) {
    // These cases intentionally panic because their outputs depend on the model,
    // not on the image file input, and invalid model format is not recoverable.
    let output = model
        .forward_is(&[IValue::Tensor(image.shallow_clone())])
        .unwrap();

    let mut results = match output {
        IValue::Tuple(elements) if elements.len() == 2 => elements,
        _ => unreachable!("expected (last_hidden_state, pooler_output)"),
    };

    let mut results = results.drain(..);

    match (results.next(), results.next()) {
        (Some(IValue::Tensor(last_hidden_state)), Some(IValue::Tensor(pooler_output))) => {
            (last_hidden_state, pooler_output)
        }
        _ => unreachable!("expected 2-tuple of tensors"),
    }
}

fn scaled_result(pooler_output: &Tensor) -> Tensor {
    let scaled_norm = pooler_output.norm().pow_tensor_scalar(-1);
    pooler_output.multiply(&scaled_norm)
}

pub fn get_model_result<R>(
    image: R,
    model: &CModule,
    device: Device,
) -> Result<ModelResult, FeatureExtractionError>
where
    R: std::io::Read + std::io::Seek,
{
    // Get image and and dimensions for calculation.
    let image = io::load_image(image, device)?;

    // Features are unstable across different global scales, and
    // somewhat stable across dimensional scales.
    //
    // Use 18 (252x252) instead of 16 (224x224) to produce a more detailed
    // result and attention map at almost exactly the same computational cost.
    //
    // It is possible for highly non-square models to have meaningful feature extraction,
    // but in practice it makes no difference identifying scales which keep the aspect
    // ratio, and does not produce feature vectors which are similar enough to identify
    // crops.
    let image_scale = 1;
    let patches = (18 * image_scale, 18 * image_scale);

    // Scale image into appropriate shape.
    let image = io::resize_image_by_patch_count(image, patches, PATCH_DIM);

    // The pooler output is the [CLS] token generated by the model.
    // It contains high-quality, robust features from the input image.
    let (last_hidden_state, pooler_output) = infer(&image, model);

    Ok(ModelResult {
        patches,
        image,
        features: scaled_result(&pooler_output.squeeze()),
        last_hidden_state,
    })
}

pub struct Executor {
    device: Device,
    model: CModule,
}

impl Executor {
    pub fn new(model_path: &str) -> Option<Self> {
        let (device, model) = io::device_and_model(model_path)?;
        Some(Self { device, model })
    }

    pub fn extract(&self, image: &[u8]) -> Result<Vec<f32>, FeatureExtractionError> {
        let image = Cursor::new(image);
        let model_result = get_model_result(image, &self.model, self.device)?;
        let features = model_result
            .features
            .iter::<f64>()
            .unwrap()
            .map(|f| f as f32)
            .collect();

        Ok(features)
    }
}
